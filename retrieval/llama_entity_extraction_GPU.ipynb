{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcd783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.2 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571529a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a917dd",
   "metadata": {},
   "source": [
    "## load questions and sql queries in spider train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903336d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your S3 bucket and file key\n",
    "bucket_name = 'sagemaker-studio-423623869859-3no3d9ie4hx'\n",
    "file_key = 'train_spider.json'  # replace with your actual file path\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Load the file from S3\n",
    "obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "json_train = json.load(obj['Body'])\n",
    "\n",
    "# Load questions and answers into a pandas dataframe\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "list_questions_and_sqls = []\n",
    "\n",
    "for ele in json_train:\n",
    "    list_questions_and_sqls.append([ele['question'], ele['query']])\n",
    "    \n",
    "df_questions_and_sqls = pd.DataFrame(list_questions_and_sqls, columns=['question', 'sql_query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68fc57",
   "metadata": {},
   "source": [
    "## falcon-7b entity extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9a751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb777d851714baf9e8458552fcb4210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model identifier from Hugging Face\n",
    "model_id_falcon = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer_falcon = AutoTokenizer.from_pretrained(model_id_falcon)\n",
    "model_falcon = AutoModelForCausalLM.from_pretrained(model_id_falcon, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe_falcon = pipeline(\"text-generation\", model=model_falcon, tokenizer=tokenizer_falcon, device=0)\n",
    "\n",
    "def extract_entities_falcon(question):\n",
    "    # Define a batch of input prompts\n",
    "    beg_time = time.time()\n",
    "\n",
    "    # Generate text for each input in the batch\n",
    "    outputs = pipe_falcon(question, max_length=512, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "    # Print the generated outputs for each input\n",
    "    #print(f\"Input: {question}\")\n",
    "    #print(f\"Output: {outputs[0]['generated_text']}\\n\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('total time spent is {}'.format(end_time-beg_time))\n",
    "    return outputs[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b2087",
   "metadata": {},
   "source": [
    "## llama 3.2 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enter your Hugging Face token here\n",
    "# login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define model identifier and access token\n",
    "# model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# # Load the tokenizer and model using the access token\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# def extract_entities_llama(question):\n",
    "#     beg_time = time.time()\n",
    "\n",
    "#     # Tokenize the input\n",
    "#     inputs = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#     # Generate text\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(\n",
    "#             **inputs,\n",
    "#             max_length=512\n",
    "#         )\n",
    "\n",
    "#     # Decode and print the output\n",
    "#     generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     end_time = time.time()\n",
    "#     print('total time spent is {} seconds'.format(end_time - beg_time))\n",
    "#     #print(question)\n",
    "#     #print(generated_text)\n",
    "    \n",
    "#     return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805526de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_map(question):\n",
    "    \n",
    "    returned_question = f\"\"\"The task is to extract the entities from the question \\\n",
    "    enclosed in double quotes at the end of this paragraph. \\\n",
    "    Only extract entities that are helpful to identify \\\n",
    "    relevant table names and column names in a database. \\\n",
    "    The table names and column names will be used in a SQL query. \\\n",
    "    Return your answer in a python dictionary whose keys are \\\n",
    "    entities_for_tables and entities_for_columns. \\\n",
    "    The value for each key will be a list of extracted entities. \\\n",
    "    Return the python dictionary only without anything extra. \\\n",
    "    For example, if the question is \"How many heads of the departments are older than 56?\", \\\n",
    "    the returned python dictionary should be \\\n",
    "    \"{{\\\"entities_for_tables\\\": [\\\"departments\\\"], \\\"entities_for_columns\\\": [\\\"heads\\\", \\\"age\\\"]}}\". \\\n",
    "    Now here is the question that you should extract entities from: \\\"{question}\\\"\\\n",
    "    \"\"\"\n",
    "\n",
    "    return returned_question\n",
    "\n",
    "df_questions_and_sqls['question_for_entity_extraction'] = df_questions_and_sqls['question'].apply(question_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254dd723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_and_sqls.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcca7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = extract_entities_falcon(df_questions_and_sqls.loc[1, 'question_for_entity_extraction'])\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231948e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
